import pandas as pd
import numpy as np
import json
import re
from utils.metrics import MSE, MAE


def parse_prediction_from_answer(answer_text):
    """
    ä»answeræ–‡æœ¬ä¸­è§£æé¢„æµ‹å€¼
    
    Args:
        answer_text: answerå­—æ®µçš„æ–‡æœ¬å†…å®¹
        
    Returns:
        dates: æ—¥æœŸåˆ—è¡¨
        values: é¢„æµ‹å€¼åˆ—è¡¨
    """
    dates = []
    values = []
    
    # æå–```ä¹‹é—´çš„å†…å®¹
    pattern = r'```(.*?)```'
    matches = re.findall(pattern, answer_text, re.DOTALL)
    
    if matches:
        content = matches[0].strip()
        lines = content.split('\n')
        
        for line in lines:
            line = line.strip()
            if line and not line.startswith('<') and not line.startswith('>'):
                # è§£ææ¯ä¸€è¡Œ: æ—¥æœŸ  å€¼
                parts = line.split()
                if len(parts) >= 2:
                    try:
                        # æ—¥æœŸæ ¼å¼: YYYY-MM-DD HH:MM:SS
                        date_str = parts[0] + ' ' + parts[1]
                        value = float(parts[2])
                        dates.append(date_str)
                        values.append(value)
                    except (ValueError, IndexError):
                        continue
    
    return dates, values


def load_ground_truth(data_name, attr, look_back, pred_window, number):
    """
    åŠ è½½çœŸå®å€¼æ•°æ®
    
    Args:
        data_name: æ•°æ®é›†åç§° (å¦‚ 'ETTh1')
        attr: å±æ€§åç§° (å¦‚ 'HUFL')
        look_back: å›çœ‹çª—å£å¤§å°
        pred_window: é¢„æµ‹çª—å£å¤§å°
        number: æ ·æœ¬åºå· (0-9)
        
    Returns:
        dates: çœŸå®å€¼å¯¹åº”çš„æ—¥æœŸ
        true_values: çœŸå®å€¼
    """
    data_dir = f'/data/songliv/TS/datasets/Single-mode/{data_name}.csv'
    data = pd.read_csv(data_dir)
    
    # æˆªå–ç›¸å…³æ•°æ®æ®µ
    data = data[12 * 30 * 24 + 4 * 30 * 24 - look_back : 12 * 30 * 24 + 8 * 30 * 24]
    
    # è·å–æŒ‡å®šæ ·æœ¬çš„çœŸå®å€¼
    # è®­ç»ƒæ•°æ®: number * look_back : (number + 1) * look_back
    # æµ‹è¯•æ•°æ®(çœŸå®å€¼): (number + 1) * look_back : (number + 1) * look_back + pred_window
    start_idx = (number + 1) * look_back
    end_idx = start_idx + pred_window
    
    true_data = data.iloc[start_idx:end_idx]
    dates = true_data['date'].tolist()
    true_values = true_data[attr].tolist()
    
    return dates, true_values


def evaluate_single_result(result_file, data_name, attr, look_back, pred_window):
    """
    è¯„ä¼°å•ä¸ªç»“æœæ–‡ä»¶
    
    Args:
        result_file: ç»“æœJSONæ–‡ä»¶è·¯å¾„
        data_name: æ•°æ®é›†åç§°
        attr: å±æ€§åç§°
        look_back: å›çœ‹çª—å£å¤§å°
        pred_window: é¢„æµ‹çª—å£å¤§å°
        
    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    # è¯»å–é¢„æµ‹ç»“æœ
    with open(result_file, 'r') as f:
        results = json.load(f)
    
    evaluation_results = []
    
    for item in results:
        index = item['index']
        answer = item.get('answer', '')
        
        # è§£æé¢„æµ‹å€¼
        # ç¡®ä¿answeræ˜¯å­—ç¬¦ä¸²ç±»å‹
        if isinstance(answer, dict):
            answer_text = str(answer)
        elif isinstance(answer, list):
            answer_text = str(answer)
        elif answer is None:
            print(f"Warning: answer is None for index {index}")
            answer_text = ""
        else:
            answer_text = str(answer)
            
        try:
            pred_dates, pred_values = parse_prediction_from_answer(answer_text)
        except Exception as e:
            print(f"Error parsing answer for index {index}: {e}")
            print(f"Answer type: {type(answer)}")
            print(f"Answer content: {answer_text[:200]}...")  # åªæ˜¾ç¤ºå‰200ä¸ªå­—ç¬¦
            pred_dates, pred_values = [], []
        original_pred_len = len(pred_values)  # ä¿å­˜åŸå§‹é¢„æµ‹é•¿åº¦
        
        # åŠ è½½çœŸå®å€¼
        true_dates, true_values = load_ground_truth(data_name, attr, look_back, pred_window, index)
        
        # è®¡ç®—é•¿åº¦å·®å¼‚
        length_diff = original_pred_len - len(true_values)
        length_status = "åŒ¹é…" if length_diff == 0 else ("è¿‡é•¿" if length_diff > 0 else "è¿‡çŸ­")
        
        # æ£€æŸ¥é•¿åº¦æ˜¯å¦åŒ¹é…
        if len(pred_values) != len(true_values):
            if length_diff > 0:
                print(f"âš ï¸  Warning: Index {index} - é¢„æµ‹å€¼è¿‡é•¿! é¢„æµ‹é•¿åº¦ {len(pred_values)} > çœŸå®é•¿åº¦ {len(true_values)} (å¤šäº† {length_diff} ä¸ª)")
            else:
                print(f"âš ï¸  Warning: Index {index} - é¢„æµ‹å€¼è¿‡çŸ­! é¢„æµ‹é•¿åº¦ {len(pred_values)} < çœŸå®é•¿åº¦ {len(true_values)} (å°‘äº† {-length_diff} ä¸ª)")
            # å–æœ€å°é•¿åº¦è¿›è¡Œè¯„ä¼°
            min_len = min(len(pred_values), len(true_values))
            pred_values = pred_values[:min_len]
            true_values = true_values[:min_len]
        
        # è®¡ç®—æŒ‡æ ‡
        if len(pred_values) > 0 and len(true_values) > 0:
            mse = MSE(true_values, pred_values)
            mae = MAE(true_values, pred_values)
            rmse = np.sqrt(mse)
            
            evaluation_results.append({
                'index': index,
                'mse': mse,
                'mae': mae,
                'rmse': rmse,
                'pred_length': original_pred_len,
                'true_length': len(true_values),
                'length_diff': length_diff,
                'length_status': length_status
            })
        else:
            print(f"âš ï¸  Warning: Index {index} - no valid predictions or true values")
            
            evaluation_results.append({
                'index': index,
                'mse': None,
                'mae': None,
                'rmse': None,
                'pred_length': original_pred_len,
                'true_length': len(true_values),
                'length_diff': length_diff,
                'length_status': length_status
            })
    
    return evaluation_results


def evaluate_all_results(result_dir, data_name, look_back=96, pred_window=96):
    """
    è¯„ä¼°æ‰€æœ‰ç»“æœæ–‡ä»¶
    
    Args:
        result_dir: ç»“æœç›®å½•è·¯å¾„
        data_name: æ•°æ®é›†åç§°
        look_back: å›çœ‹çª—å£å¤§å°
        pred_window: é¢„æµ‹çª—å£å¤§å°
        
    Returns:
        æ‰€æœ‰è¯„ä¼°ç»“æœçš„æ±‡æ€»
    """
    import os
    
    attrs = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']
    
    all_evaluations = {}
    
    # æŒ‰å±æ€§åˆ†ç»„ï¼Œæ”¶é›†æ‰€æœ‰ç‰‡æ®µçš„ç»“æœ
    for attr in attrs:
        print(f"\n{'='*60}")
        print(f"Evaluating {attr} - Collecting all segment results...")
        print(f"{'='*60}")
        
        attr_all_results = []  # æ”¶é›†è¯¥å±æ€§çš„æ‰€æœ‰ç»“æœ
        
        # éå†æ‰€æœ‰ç‰‡æ®µ
        for i in range(88):
            result_file = os.path.join(result_dir, f'result_{attr}_{data_name}_{look_back}_{pred_window}_{i}.json')
            
            if os.path.exists(result_file):
                print(f"  Processing segment {i}...")
                eval_results = evaluate_single_result(result_file, data_name, attr, look_back, pred_window)
                attr_all_results.extend(eval_results)
            else:
                print(f"  Segment {i} not found, skipping...")
        
        # è®¡ç®—è¯¥å±æ€§çš„æ±‡æ€»æŒ‡æ ‡
        valid_results = [r for r in attr_all_results if r['mse'] is not None]
        
        if valid_results:
            avg_mse = np.mean([r['mse'] for r in valid_results])
            avg_mae = np.mean([r['mae'] for r in valid_results])
            avg_rmse = np.mean([r['rmse'] for r in valid_results])
            
            all_evaluations[attr] = {
                'individual_results': attr_all_results,
                'average_mse': avg_mse,
                'average_mae': avg_mae,
                'average_rmse': avg_rmse,
                'num_samples': len(valid_results),
                'total_segments': len(attr_all_results)
            }
            
            print(f"\nğŸ“Š Final Results for {attr}:")
            print(f"  Average MSE:  {avg_mse:.6f}")
            print(f"  Average MAE:  {avg_mae:.6f}")
            print(f"  Average RMSE: {avg_rmse:.6f}")
            print(f"  Valid samples: {len(valid_results)}/{len(attr_all_results)}")
            print(f"  Available segments: {len(attr_all_results)}/118")
            
            # æ‰“å°æ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†ç»“æœ
            print(f"\n  Individual samples:")
            for r in attr_all_results:
                if r['mse'] is not None:
                    length_info = f"[{r['length_status']}]" if r['length_status'] != "åŒ¹é…" else ""
                    print(f"    Index {r['index']}: MSE={r['mse']:.6f}, MAE={r['mae']:.6f}, RMSE={r['rmse']:.6f} {length_info}")
                    if r['length_status'] != "åŒ¹é…":
                        print(f"               é•¿åº¦: é¢„æµ‹={r['pred_length']}, çœŸå®={r['true_length']}, å·®å¼‚={r['length_diff']:+d}")
                else:
                    print(f"    Index {r['index']}: No valid evaluation")
        else:
            print(f"âš ï¸  No valid results for {attr}")
            all_evaluations[attr] = None
    
    return all_evaluations


def print_summary(all_evaluations):
    """
    æ‰“å°è¯„ä¼°ç»“æœæ‘˜è¦
    """
    print(f"\n{'='*60}")
    print("ğŸ“ˆ EVALUATION SUMMARY")
    print(f"{'='*60}\n")
    
    print(f"{'Attribute':<10} {'Avg MSE':<15} {'Avg MAE':<15} {'Avg RMSE':<15} {'Samples':<10}")
    print("-" * 65)
    
    for attr, results in all_evaluations.items():
        if results is not None:
            print(f"{attr:<10} {results['average_mse']:<15.6f} {results['average_mae']:<15.6f} "
                  f"{results['average_rmse']:<15.6f} {results['num_samples']:<10}")
        else:
            print(f"{attr:<10} {'N/A':<15} {'N/A':<15} {'N/A':<15} {'N/A':<10}")
    
    print("-" * 65)
    
    # è®¡ç®—æ€»ä½“å¹³å‡
    valid_attrs = [k for k, v in all_evaluations.items() if v is not None]
    if valid_attrs:
        overall_mse = np.mean([all_evaluations[k]['average_mse'] for k in valid_attrs])
        overall_mae = np.mean([all_evaluations[k]['average_mae'] for k in valid_attrs])
        overall_rmse = np.mean([all_evaluations[k]['average_rmse'] for k in valid_attrs])
        
        print(f"{'Overall':<10} {overall_mse:<15.6f} {overall_mae:<15.6f} {overall_rmse:<15.6f}")
    
    print(f"\n{'='*60}\n")


if __name__ == "__main__":
    # è¯„ä¼°ETTh1æ•°æ®é›†çš„ç»“æœ
    data_name = 'ETTh1'
    # for i in [7]:
    #     result_dir = f'/data/songliv/TS/TimeReasoner/output_{i}/result/{data_name}'
    #     output_file = f'/data/songliv/TS/TimeReasoner/output_{i}/evaluation_results.json'
    result_dir = f'/data/songliv/TS/TimeReasoner/output_3_summery/result/{data_name}'
    output_file = f'/data/songliv/TS/TimeReasoner/output_3_summery/evaluation_results.json'
    print(result_dir)
    look_back = 96
    pred_window = 96
    
    all_evaluations = evaluate_all_results(result_dir, data_name, look_back, pred_window)
    print_summary(all_evaluations)
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    
    with open(output_file, 'w') as f:
        # è½¬æ¢numpyç±»å‹ä¸ºpythonåŸç”Ÿç±»å‹ä»¥ä¾¿JSONåºåˆ—åŒ–
        serializable_results = {}
        for attr, results in all_evaluations.items():
            if results is not None:
                serializable_results[attr] = {
                    'average_mse': float(results['average_mse']),
                    'average_mae': float(results['average_mae']),
                    'average_rmse': float(results['average_rmse']),
                    'num_samples': int(results['num_samples']),
                    'individual_results': [
                        {
                            'index': int(r['index']),
                            'mse': float(r['mse']) if r['mse'] is not None else None,
                            'mae': float(r['mae']) if r['mae'] is not None else None,
                            'rmse': float(r['rmse']) if r['rmse'] is not None else None,
                            'pred_length': int(r['pred_length']),
                            'true_length': int(r['true_length']),
                            'length_diff': int(r['length_diff']),
                            'length_status': r['length_status']
                        }
                        for r in results['individual_results']
                    ]
                }
            else:
                serializable_results[attr] = None
        
        json.dump(serializable_results, f, indent=2)
    
    print(f"âœ… Evaluation results saved to: {output_file}")

